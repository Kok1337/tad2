{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4130dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f98203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FoodDataset(Dataset):\n",
    "    def __init__(self, data_type=None, transforms=None, size_scale=1):\n",
    "        if data_type is None:\n",
    "            raise Exception(\"You are useless\")\n",
    "        if size_scale <= 0 or size_scale > 1:\n",
    "            raise Exception(\"You are useless\")\n",
    "        self.path = 'data/Food-5K/' + data_type + '/'\n",
    "        self.images_name = os.listdir(self.path)\n",
    "        self.transforms = transforms\n",
    "        self.size_scale = size_scale\n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(len(self.images_name) * self.size_scale)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = self.images_name[idx]\n",
    "        \n",
    "        label = data.split('_')[0]\n",
    "        label = int(label)\n",
    "        label = torch.tensor(label)\n",
    "        \n",
    "        image = cv2.imread(self.path + data)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.transforms:\n",
    "            aug = self.transforms(image=image)\n",
    "            image = aug['image']\n",
    "        \n",
    "        return (image, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8dc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transformation = A.Compose([A.RandomResizedCrop(256, 256),\n",
    "                            ToTensorV2()\n",
    "                        ])\n",
    "\n",
    "image_aug_transform = A.Compose([\n",
    "                            A.Resize(256, 256),\n",
    "                            A.GaussNoise(p=1, mean=60),\n",
    "                            A.RandomBrightnessContrast(p=1, brightness_limit=0.5),\n",
    "                            ToTensorV2()\n",
    "                        ])\n",
    "\n",
    "train_dataset = FoodDataset('training', image_transformation)\n",
    "valid_dataset = FoodDataset('validation', image_transformation)\n",
    "test_dataset = FoodDataset('evaluation', image_transformation)\n",
    "\n",
    "train_aug_dataset = FoodDataset('training', image_aug_transform, 0.3)\n",
    "valid_aug_dataset = FoodDataset('validation', image_aug_transform, 0.3)\n",
    "test_aug_dataset = FoodDataset('evaluation', image_aug_transform, 0.3)\n",
    "\n",
    "train_aug_dataset = torch.utils.data.ConcatDataset([train_aug_dataset, train_dataset])\n",
    "valid_aug_dataset = torch.utils.data.ConcatDataset([valid_aug_dataset, valid_dataset])\n",
    "test_aug_dataset = torch.utils.data.ConcatDataset([test_aug_dataset, test_dataset])\n",
    "\n",
    "trainloader = DataLoader(train_aug_dataset, batch_size=32, shuffle=True)\n",
    "validloader = DataLoader(valid_aug_dataset, batch_size=32)\n",
    "testloader = DataLoader(test_aug_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee424029",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(average='macro', num_classes=6)\n",
    "        self.recall = torchmetrics.Recall(average='macro', num_classes=6)\n",
    "        self.f1 = torchmetrics.F1Score(average='macro', num_classes=6)\n",
    "        self.precision_metrics = torchmetrics.Precision(average='macro', num_classes=6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.model(x)\n",
    "        return y_pred\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=0.008)\n",
    "        return optimizer\n",
    "    \n",
    "    def get_metrics(self, y_pred, y, metrics_type=\"train\"):\n",
    "        predictions = y_pred.argmax(-1)\n",
    "        y = y.type(torch.IntTensor)\n",
    "        \n",
    "        precision = self.precision_metrics(predictions, y)\n",
    "        acc = self.accuracy(predictions, y)\n",
    "        rec = self.recall(predictions, y)\n",
    "        f1 = self.f1(predictions, y)\n",
    "\n",
    "        self.log('acc/test', acc)\n",
    "        self.log('rec/test', rec)\n",
    "        self.log('f1/test', f1)\n",
    "        self.log('prec/test', precision)\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        y_pred = self.forward(x.float()).squeeze()\n",
    "        # y_pred = torch.unsqueeze(y_pred, 0)\n",
    "        loss = self.loss(y_pred, y)\n",
    "        \n",
    "        self.get_metrics(y_pred, y, \"train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "\n",
    "        y_pred = self.forward(x.float()).squeeze()\n",
    "        # y_pred = torch.unsqueeze(y_pred, 0)\n",
    "        loss = self.loss(y_pred, y)\n",
    "\n",
    "        self.get_metrics(y_pred, y, \"val\")\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        y_pred = self.forward(x.float()).squeeze()\n",
    "        # y_pred = torch.unsqueeze(y_pred, 0)\n",
    "        loss = self.loss(y_pred, y)\n",
    "\n",
    "        self.get_metrics(y_pred, y, \"test\")\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a804a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(resnet_model, model_name):\n",
    "    gpu = 1 if torch.cuda.is_available() else 0\n",
    "    resnet_model.fc = nn.Linear(resnet_model.fc.in_features, 2)\n",
    "    resnet_model = LitModel(model)\n",
    "    trainer_resnet_model= pl.Trainer(max_epochs=3, gpus=gpu)\n",
    "    trainer_resnet_model.fit(resnet_model, trainloader, validloader)\n",
    "    torch.save(resnet_model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c871594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eea8cd7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name              | Type             | Params\n",
      "-------------------------------------------------------\n",
      "0 | model             | ResNet           | 11.2 M\n",
      "1 | loss              | CrossEntropyLoss | 0     \n",
      "2 | accuracy          | Accuracy         | 0     \n",
      "3 | recall            | Recall           | 0     \n",
      "4 | f1                | F1Score          | 0     \n",
      "5 | precision_metrics | Precision        | 0     \n",
      "-------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.710    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c7742645f449f6b694ba726987ba33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(weights='IMAGENET1K_V1')\n",
    "train_model(model, 'resnet50_aug.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0acfe0",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f88eccce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'resnet18_aug.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresnet18_aug.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m LitModel(model)\n\u001b[0;32m      3\u001b[0m trainer_model \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mD:\\Programs\\Anaconda3\\lib\\site-packages\\torch\\serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'resnet18_aug.pth'"
     ]
    }
   ],
   "source": [
    "model = torch.load('resnet18_aug.pth', map_location=torch.device('cpu'))\n",
    "model = LitModel(model)\n",
    "trainer_model = pl.Trainer(max_epochs=20, accelerator='cpu')\n",
    "trainer_model.test(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f71ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
